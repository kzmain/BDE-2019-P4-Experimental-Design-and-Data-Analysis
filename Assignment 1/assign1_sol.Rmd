---
title: "Assignment 1"
author: "Chang Liu"
date: "2020/2/6"
output: pdf_document
---
```{r echo=FALSE, results='hide',message=FALSE}
set.seed(1234)
```
## Exercise 1
First, I construct a simple function that could return the power of t-test with given parameters (n,m,mu,nu,sd). 
```{r}
powerOfTtest <- function(n, m, mu, nu, sd)
{
  B = 1000; p = numeric(B); power = numeric(length(nu))
  for (i in 1:length(nu))
  {
    for (b in 1:B)
    {
      x = rnorm(n, mu,sd); y = rnorm(m, nu[i],sd)
      p[b] = t.test(x,y,var.equal=TRUE)[[3]]
    }
    power[i] = mean(p<0.05)
  }
  power
}
```

**a)** & **b)** &**c)**
Using the above function, I can now calculate the power of the t-test with  given parameters. Then I make a plot for all sets of parameters with red, green and blue color respectively.
```{r}
nu = seq(175,185,by=0.25)
# compute and plot power function with parametrs : n = m = 30, mu=180 and sd=5
n = m = 30; mu = 180; sd = 5
power = powerOfTtest(n, m, mu, nu, sd)
plot(nu, power, col = 'red')

# compute and plot power function with parametrs : n = m = 100, mu=180 and sd=5.
n = m = 100; mu = 180; sd = 5
power = powerOfTtest(n, m, mu, nu, sd)
points(nu, power, col = 'green')

# compute power function with parametrs : n = m = 30, mu=180 and sd=15.
n = m = 30; mu = 180; sd = 15
power = powerOfTtest(n, m, mu, nu, sd)
points(nu, power, col = 'blue')
```

**d)**
First, with fixed n,m,sd and mu. As the second sample's mean of the sampling distribution (nu) goes closer to mu, the power of p-value could be rather low which suggests t-test tends to NOT rejects the null hypothesis and gives the right result. Second, comparing plot from problem a and b, b's up-side-down bell-shaped plot is thinner than a's plot, indicating as sample size increase t-test becomes more strict, t-test will NOT reject null hypothesis only when two means fairly close to each other. The third conclusion comes from problem c, when standard deviation becomes larger, its plot became less smooth and t-test's performance became unstable. Because higher sample sizes yield higher power, increasing sample size may solve this problem.

## Exercise 2
**a)** 
To investigate the normality for all three data sets, I choose to use Shapiro-Wilk normality test with an alpha level of 0.05. The null-hypothesis of this test is that the population is normally distributed.
```{r}
light = 7.442 / ((scan('light.txt', quiet=TRUE) / 1000 + 24.8) / 10^6)
light1879 = scan('light1879.txt', quiet=TRUE) + 299000
light1882 = scan('light1882.txt', quiet=TRUE) + 299000
par(mfrow=c(1,3))
shapiro.test(light)[[2]]
shapiro.test(light1879)[[2]]
shapiro.test(light1882)[[2]]
```
After computing all three data sets' p-value, the data from light1879 and light1882 have a p-value greater than 0.05. So I would deduce 'light1879' and 'light1882' is normally distributed, the data set 'light' is not normally distributed.

**b)**
We can use t-distribution to calculate distribution confidence intervals even the distribution is not a normal distribution. t.test in R would give me confidence intervals directly.
```{r}
lightCi = t.test(light)[[4]]
light1879Ci = t.test(light1879)[[4]]
light1882Ci = t.test(light1882)[[4]]
c(lightCi[1], lightCi[2])
c(light1879Ci[1], light1879Ci[2])
c(light1882Ci[1], light1882Ci[2])
```

**c)** 
t-test will perform less accurately when the distribution is not normal, so I choose Wilcoxon signed rank test to find the first sample's p-value.
```{r warning=FALSE}
lightSpeed = 299792.458
p = wilcox.test(light,mu=lightSpeed)[[3]]
p1879 = t.test(light1879, mu=lightSpeed)[[3]]
p1882 = t.test(light1882,mu=lightSpeed)[[3]]
p; p1879; p1882;
```
As reult, only light1882's p-vlaue (0.1189198) > 0.05, which makes it most accurate sample.

## Exercise 3
**a)**
```{r}
telephoneBills = read.table('telephone.txt', header=TRUE)
telephone =  telephoneBills[telephoneBills$Bills!=0, ]
hist(telephone, xlab='First Month Bills', ylab='Money', main='Histogram of New Subscribers')
```
I think best plot to represent the distribution of subscribers is histogram.
The strategy that the manager should adopt is to increase the preferential activities in the price range of 30-70 to promote low consumption users to increase consumption, and ultimately increase users in this price range.
There is one inconsistencies in the data. It contains zero value which is inappropriate since the survey should target customers who have already spent. Hence zero-cost subscribers shouldn't be included.

**b)**
I make a sequence between 0.01 and 0.1, and use bootstrap-test to test every single one of them to evaluate wether data fits exponential distribution.
```{r}
lambdas=seq(0.01, 0.1, by=0.01)
B=1000
t=median(telephone)
n=length(telephone)
p=numeric(length(lambdas))
for (i in 1:length(lambdas)) {
  tstar=numeric(B)
  for (b in 1:B) {
    xstar=rexp(n, lambdas[i])
    tstar[b]=median(xstar)
  }
  pl=sum(tstar<t)/B
  pr=sum(tstar>t)/B
  p[i]=2*min(pl,pr)
}
p
```
As result, when lambda equals 0.02, p-value = 0.074 > 0.05, so the data fit Exp(0.02)

**c)**
To construct a 95% bootstrap confidence interval, first I should do bootstrap simulation to generate 1000 groups $(X_{1}^{*}, \ldots, X_{N}^{*})$ and compute $T_{i}^{*}=median\left(X_{1}^{*}, \ldots, X_{N}^{*}\right)$. With the formula for the bootstrap confidence interval with confidence $1-2\alpha$: $\left[2 T-T_{(1-\alpha)}^{*}, 2 T-T_{(\alpha)}^{*}\right]$, I can now construct a 95% bootstrap confidence interval.
```{r}
B = 1000
medians = numeric(B)
for (b in 1:B) {
  xstar=sample(telephone, size=length(telephone), replace=TRUE)
  medians[b] = median(xstar)
}
Tstar25 = quantile(medians, 0.025)
Tstar975 = quantile(medians, 0.975)
T1 = median(telephone)
c(2*T1-Tstar975,2*T1-Tstar25)
```

**d)**
For an exponential distribution, we have $E[X]=\frac{1}{\lambda}$. When applying bootstrap to simulate central limit theorem to an exponential distribution, we could expect ${\hat\lambda} = \frac{1}{\bar{X}}$.
```{r}
B = 1000
sample_means = numeric(B)
medians = numeric(B)
for (b in 1:B) {
  xstar = sample(telephone, size=length(telephone), replace=TRUE)
  sample_means[b] = mean(xstar)
  medians[b] = median(xstar)
}

central = mean(sample_means)
lambda = 1 / central
lambda
Tstar25 = quantile(medians, 0.025)
Tstar975 = quantile(medians, 0.975)
T1 = median(telephone)
c(2*T1-Tstar975,2*T1-Tstar25)
```
So we have $\lambda$ = 0.022, and CI for population median is [16.12500, 36.56688].

**e)**
I choose sign test to verify wheter the median is bigger or equal to 40, and wheter the probability less than 10 is at most 25%.
```{r}
binom.test(sum(telephone>=40),length(telephone),p=0.5)[[3]]
binom.test(sum(telephone<10),length(telephone),p=0.25, alternative='greater')[[3]]
```
the p-value of first test 0.07091956 > 0.05. Conclusion: H0 is not rejected, median bill is bigger or equal to 40 euro.
the p-value of first test 0.7715 > 0.05. Conclusion: H0 is not rejected, the fraction of bills less than 10 euro is at most 25%.

## Exercise 4
**a)**
```{r}
run = read.table('run.txt')
cor.test(run$before, run$after)

qqnorm(run$before)
qqnorm(run$after)
shapiro.test(run$before)[[2]]
shapiro.test(run$after)[[2]]
```
For both two columns' data are nomally distributed, we could use Pearon's to test whether run times before drink and after are correlated, and finally we got p-value = 0.00078, which is much smaller than 0.05.

Moreover, we ran Shapiro-Wilk normality test and drew QQ-Plot to check both the "before runing data" and the "after running data"'s normlity.

As both data Shapiro test p-value results are bigger than 0.05 and QQ-Plots' dots are both nearly in a line, both tow columns' data are normally distributed.

Conclusion: there is significant correlation, given dataset's normality.

**b)**
For difference in speed test we will use t-tset in both softdrink and the energy drink conditions. The test  null hypothesis: $H_{0}:H_{before} = H_{after}$, and alternative hypothesis: $H_{before} \neq H_{after}$.
```{r}
lemo = run[run$drink=='lemo', ]
t.test(lemo$before, lemo$after, paired = TRUE)[[3]]
qqnorm(lemo$after - lemo$before)
```

```{r}
energy = run[run$drink=='energy', ]
t.test(energy$before, energy$after, paired = TRUE)[[3]]
qqnorm(energy$after - energy$before)
```
From the paired t-test's requirement, difference between run before and run after should be normally distributed, however from the QQ-plot it shows that neither soft drink' nor energy drink's differences are normally distributed. So we will try to use permutation tests, which normality of data differencee is not required.
```{r}
 mystat=function(x,y) {mean(x-y)} 
B=1000
tstar=numeric(B)
for (i in 1:B) 
{ 
 lemonstar=t(apply(cbind(lemo$before,lemo$after),1,sample))
 tstar[i]=mystat(lemonstar[,1],lemonstar[,2]) 
}
 myt=mystat(lemo$before,lemo$after)
 pl=sum(tstar<myt)/B 
 pr=sum(tstar>myt)/B
 p=2*min(pl,pr)
 p
```
```{r}
mystat=function(x,y) {mean(x-y)} 
B=1000
tstar=numeric(B)
for (i in 1:B) 
{ 
 lemonstar=t(apply(cbind(energy$before,energy$after),1,sample))
 tstar[i]=mystat(lemonstar[,1],lemonstar[,2]) 
}
 myt=mystat(energy$before,energy$after)
 pl=sum(tstar<myt)/B 
 pr=sum(tstar>myt)/B
 p=2*min(pl,pr)
 p
```

Both two permutation tests results (softdrink: 0.434 and energy: 0.136) are bigger than 0.05, which means there are no big speed difference before drink and and after, no matter we test on which drink.

A more interesing outcome is t.test of softdrink's p-value = 0.4373423 > 0.05 and t.test of energy's p-value = 0.1263962 > 0.05, which means t-test results also show no big speed difference before drink and and after, both in softdrink condition and energy drink condition.

**c)**
We chose permutation test and $T_{i}^{*}=mean(X^{*}-Y^{*})$ to test whether these time
differences are effected by the type of drink.
```{r}
lemo$difference = lemo$before - lemo$after
energy$difference = energy$before - energy$after
mystat=function(x,y) {mean(x-y)}
B=1000
tstar=numeric(B)
for (i in 1:B) {
  adiffstar=t(apply(cbind(lemo$difference,energy$difference),1,sample))
  tstar[i]=mystat(adiffstar[,1],adiffstar[,2])
}
myt=mystat(lemo$difference,energy$difference)
pl=sum(tstar<myt)/B
pr=sum(tstar>myt)/B
p=2*min(pl,pr)
p
```
p-value = 0.188 > 0.05, hence the is no difference between the two types of drinks.

**d)**
Whether drinking the energy drink speeds up the running have another important attribute, that is the test time after drinking. Maybe energy drink will have great influence just in a very short time, may be 5 min, after it drinked. So more test cases on test time are needed. This is the similar objection to the the experiment design in c).

## Exercise 5
**a)**
```{r}
meatmeal = chickwts[chickwts$feed == 'meatmeal', ]$weight
sunflower = chickwts[chickwts$feed == 'sunflower', ]$weight
t.test(meatmeal, sunflower)[[3]]
wilcox.test(meatmeal, sunflower)[[3]]
ks.test(meatmeal, sunflower)[[2]]

qqnorm(meatmeal)
qqnorm(sunflower)
```
For paired t-test argues that the two outcomes are measured on the same experimental unit, but it is not in this case, so the data is not paired.

After we ran the tests, we got t-test reulst: 0.04441462, Mann-Whitney test result: 0.06881704, Kolmogorov-Smirnov test result: 0.108496. 

From, t-test result, we can assume that meatmeal condition and sunflower condition has great difference of mean weight, but this assumption is not correct. This is becuase from QQ-plot of sunflower we can find that the data is not normally distributed.

Both Mann-Whitney test and Kolmogorov-Smirnov test show that there are no big difference between meatmeal condition and sunflower condition.

But why MW got bigger p-value than KS's p-value?The KS test is sensitive to any differences in the two distributions. Substantial differences in shape, spread or median will result in a small P value. In contrast, the MW test is mostly sensitive to changes in the median. This means in this question we'd better take MW's p-value.

**b)**
```{r}
chickwtsaov=lm(weight~feed,data=chickwts)
anova(chickwtsaov)[[5]][1]
summary(chickwtsaov)
```
After ran the script, we got one-way ANOVA's p-value=5.93642e-10 < 0.05, so we can reach a conclusion that there do have difference between each group.

The summary of ANOVA shows estimated chick weights for each feed supplements are 323.583(casein), -163.383+323.583=160.2(horsebean), 218.75(linseed), 276.909(meatmeal), 246.428(soybean) and 328.916(sunflower). 

In conclusion sunflower is the best feed supplement.
```{r}
par(mfrow=c(1,1)); qqnorm(residuals(chickwtsaov))
```
Finally we need to check the assumption of normality of the populations. We used residuals data to draw QQ-plots, and all data are in a line, which is normality.

**c)**
```{r}
X <- split(chickwts$weight, chickwts$feed)
boxplot(X)
stripchart(X,vertical=TRUE)
```
I choose boxplot and strip to check the ANOVA model assumptions. In boxplot graph both casein and sunflower condition got the highest middian. From the strip chart we found that casein condition distributed more widely, in contranst sunflower condition's data is more concentrated. These mean out conclusion in b) is correct.

**d)**
```{r}
attach(chickwts)
kruskal.test(weight,feed)
```
The Kruskal-Wallis test p-value is 5.113e-07< 0.05, so we can reach a conclusion that there do have difference between each group. Furthermore, this conclusion is the same as b).